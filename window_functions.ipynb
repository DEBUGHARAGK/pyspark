{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"window_functions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number,rank,dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   name|department|salary|\n",
      "+-------+----------+------+\n",
      "|  Alice|     Sales|  5000|\n",
      "|    Bob|     Sales|  4800|\n",
      "|Charlie|        IT|  6000|\n",
      "|  David|        IT|  5800|\n",
      "|    Eve|     Sales|  5000|\n",
      "|  Frank|        IT|  6000|\n",
      "|  Grace|        IT|  5800|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice\", \"Sales\", 5000),\n",
    "    (\"Bob\", \"Sales\", 4800),\n",
    "    (\"Charlie\", \"IT\", 6000),\n",
    "    (\"David\", \"IT\", 5800),\n",
    "    (\"Eve\", \"Sales\", 5000),\n",
    "    (\"Frank\", \"IT\", 6000),\n",
    "    (\"Grace\", \"IT\", 5800)\n",
    "]\n",
    "\n",
    "schema = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('department').orderBy('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|row_number|\n",
      "+-------+----------+------+----------+\n",
      "|  David|        IT|  5800|         1|\n",
      "|  Grace|        IT|  5800|         2|\n",
      "|Charlie|        IT|  6000|         3|\n",
      "|  Frank|        IT|  6000|         4|\n",
      "|    Bob|     Sales|  4800|         1|\n",
      "|  Alice|     Sales|  5000|         2|\n",
      "|    Eve|     Sales|  5000|         3|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_row_num = df.withColumn('row_number', row_number().over(window))\n",
    "\n",
    "df_row_num.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+\n",
      "|   name|department|salary|rnk|\n",
      "+-------+----------+------+---+\n",
      "|  David|        IT|  5800|  1|\n",
      "|  Grace|        IT|  5800|  1|\n",
      "|Charlie|        IT|  6000|  3|\n",
      "|  Frank|        IT|  6000|  3|\n",
      "|    Bob|     Sales|  4800|  1|\n",
      "|  Alice|     Sales|  5000|  2|\n",
      "|    Eve|     Sales|  5000|  2|\n",
      "+-------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rank = df.withColumn('rnk', rank().over(window))\n",
    "\n",
    "df_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|dense_rank|\n",
      "+-------+----------+------+----------+\n",
      "|  David|        IT|  5800|         1|\n",
      "|  Grace|        IT|  5800|         1|\n",
      "|Charlie|        IT|  6000|         2|\n",
      "|  Frank|        IT|  6000|         2|\n",
      "|    Bob|     Sales|  4800|         1|\n",
      "|  Alice|     Sales|  5000|         2|\n",
      "|    Eve|     Sales|  5000|         2|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dense_rnk = df.withColumn('dense_rank',dense_rank().over(window))\n",
    "\n",
    "df_dense_rnk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('department').orderBy('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   name|department|salary| lag|\n",
      "+-------+----------+------+----+\n",
      "|  David|        IT|  5800|null|\n",
      "|  Grace|        IT|  5800|5800|\n",
      "|Charlie|        IT|  6000|5800|\n",
      "|  Frank|        IT|  6000|6000|\n",
      "|    Bob|     Sales|  4800|null|\n",
      "|  Alice|     Sales|  5000|4800|\n",
      "|    Eve|     Sales|  5000|5000|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lag = df.withColumn('lag',lag('salary',1).over(window))\n",
    "df_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('department').orderBy('salary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   name|department|salary|lead|\n",
      "+-------+----------+------+----+\n",
      "|  David|        IT|  5800|5800|\n",
      "|  Grace|        IT|  5800|6000|\n",
      "|Charlie|        IT|  6000|6000|\n",
      "|  Frank|        IT|  6000|null|\n",
      "|    Bob|     Sales|  4800|5000|\n",
      "|  Alice|     Sales|  5000|5000|\n",
      "|    Eve|     Sales|  5000|null|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lead = df.withColumn('lead',lead('salary',1).over(window))\n",
    "df_lead.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('department').orderBy('salary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---------+\n",
      "|   name|department|salary|first_sal|\n",
      "+-------+----------+------+---------+\n",
      "|  David|        IT|  5800|     5800|\n",
      "|  Grace|        IT|  5800|     5800|\n",
      "|Charlie|        IT|  6000|     5800|\n",
      "|  Frank|        IT|  6000|     5800|\n",
      "|    Bob|     Sales|  4800|     4800|\n",
      "|  Alice|     Sales|  5000|     4800|\n",
      "|    Eve|     Sales|  5000|     4800|\n",
      "+-------+----------+------+---------+\n",
      "\n",
      "+-------+----------+------+--------+\n",
      "|   name|department|salary|last_sal|\n",
      "+-------+----------+------+--------+\n",
      "|  David|        IT|  5800|    5800|\n",
      "|  Grace|        IT|  5800|    5800|\n",
      "|Charlie|        IT|  6000|    6000|\n",
      "|  Frank|        IT|  6000|    6000|\n",
      "|    Bob|     Sales|  4800|    4800|\n",
      "|  Alice|     Sales|  5000|    5000|\n",
      "|    Eve|     Sales|  5000|    5000|\n",
      "+-------+----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('first_sal',first('salary').over(window)).show()\n",
    "df.withColumn('last_sal',last('salary').over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy('department').orderBy('salary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   name|department|salary|\n",
      "+-------+----------+------+\n",
      "|    Bob|     Sales|  4800|\n",
      "|    Eve|     Sales|  5000|\n",
      "|  Alice|     Sales|  5000|\n",
      "|  Grace|        IT|  5800|\n",
      "|  David|        IT|  5800|\n",
      "|Charlie|        IT|  6000|\n",
      "|  Frank|        IT|  6000|\n",
      "+-------+----------+------+\n",
      "\n",
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|ntile|\n",
      "+-------+----------+------+-----+\n",
      "|  David|        IT|  5800|    1|\n",
      "|  Grace|        IT|  5800|    1|\n",
      "|Charlie|        IT|  6000|    2|\n",
      "|  Frank|        IT|  6000|    2|\n",
      "|    Bob|     Sales|  4800|    1|\n",
      "|  Alice|     Sales|  5000|    1|\n",
      "|    Eve|     Sales|  5000|    2|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('salary').show()\n",
    "df.withColumn('ntile',ntile(2).over(window)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|ntile|\n",
      "+-------+----------+------+-----+\n",
      "|  David|        IT|  5800|    1|\n",
      "|  Grace|        IT|  5800|    1|\n",
      "|Charlie|        IT|  6000|    2|\n",
      "|  Frank|        IT|  6000|    3|\n",
      "|    Bob|     Sales|  4800|    1|\n",
      "|  Alice|     Sales|  5000|    2|\n",
      "|    Eve|     Sales|  5000|    3|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ntile',ntile(3).over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|name   |department|salary|sum_salary|\n",
      "+-------+----------+------+----------+\n",
      "|David  |IT        |5800  |11600     |\n",
      "|Grace  |IT        |5800  |11600     |\n",
      "|Charlie|IT        |6000  |23600     |\n",
      "|Frank  |IT        |6000  |23600     |\n",
      "|Bob    |Sales     |4800  |4800      |\n",
      "|Alice  |Sales     |5000  |14800     |\n",
      "|Eve    |Sales     |5000  |14800     |\n",
      "+-------+----------+------+----------+\n",
      "\n",
      "+-------+----------+------+-----------------+\n",
      "|name   |department|salary|avg_salary       |\n",
      "+-------+----------+------+-----------------+\n",
      "|David  |IT        |5800  |5800.0           |\n",
      "|Grace  |IT        |5800  |5800.0           |\n",
      "|Charlie|IT        |6000  |5900.0           |\n",
      "|Frank  |IT        |6000  |5900.0           |\n",
      "|Bob    |Sales     |4800  |4800.0           |\n",
      "|Alice  |Sales     |5000  |4933.333333333333|\n",
      "|Eve    |Sales     |5000  |4933.333333333333|\n",
      "+-------+----------+------+-----------------+\n",
      "\n",
      "+-------+----------+------+----------+\n",
      "|name   |department|salary|min_salary|\n",
      "+-------+----------+------+----------+\n",
      "|David  |IT        |5800  |5800      |\n",
      "|Grace  |IT        |5800  |5800      |\n",
      "|Charlie|IT        |6000  |5800      |\n",
      "|Frank  |IT        |6000  |5800      |\n",
      "|Bob    |Sales     |4800  |4800      |\n",
      "|Alice  |Sales     |5000  |4800      |\n",
      "|Eve    |Sales     |5000  |4800      |\n",
      "+-------+----------+------+----------+\n",
      "\n",
      "+-------+----------+------+----------+\n",
      "|name   |department|salary|max_salary|\n",
      "+-------+----------+------+----------+\n",
      "|David  |IT        |5800  |5800      |\n",
      "|Grace  |IT        |5800  |5800      |\n",
      "|Charlie|IT        |6000  |6000      |\n",
      "|Frank  |IT        |6000  |6000      |\n",
      "|Bob    |Sales     |4800  |4800      |\n",
      "|Alice  |Sales     |5000  |5000      |\n",
      "|Eve    |Sales     |5000  |5000      |\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy('department').orderBy('salary')\n",
    "\n",
    "# Apply sum() function\n",
    "df_with_sum = df.withColumn(\"sum_salary\", sum(\"salary\").over(windowSpec))\n",
    "\n",
    "# Apply avg() function\n",
    "df_with_avg = df.withColumn(\"avg_salary\", avg(\"salary\").over(windowSpec))\n",
    "\n",
    "# Apply min() function\n",
    "df_with_min = df.withColumn(\"min_salary\", min(\"salary\").over(windowSpec))\n",
    "\n",
    "# Apply max() function\n",
    "df_with_max = df.withColumn(\"max_salary\", max(\"salary\").over(windowSpec))\n",
    "\n",
    "# Show the results\n",
    "df_with_sum.show(truncate=False)\n",
    "df_with_avg.show(truncate=False)\n",
    "df_with_min.show(truncate=False)\n",
    "df_with_max.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------------+\n",
      "|name   |department|salary|percent_rank      |\n",
      "+-------+----------+------+------------------+\n",
      "|David  |IT        |5800  |0.0               |\n",
      "|Grace  |IT        |5800  |0.0               |\n",
      "|Charlie|IT        |6000  |0.6666666666666666|\n",
      "|Frank  |IT        |6000  |0.6666666666666666|\n",
      "|Bob    |Sales     |4800  |0.0               |\n",
      "|Alice  |Sales     |5000  |0.5               |\n",
      "|Eve    |Sales     |5000  |0.5               |\n",
      "+-------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "# Apply percent_rank() function\n",
    "df_with_percent_rank = df.withColumn(\"percent_rank\", percent_rank().over(windowSpec))\n",
    "\n",
    "# Show the result\n",
    "df_with_percent_rank.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_project_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
