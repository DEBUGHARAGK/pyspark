{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_joins\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for DataFrame 1\n",
    "data1 = [(\"Alice\", 3000, \"HR\"),\n",
    "         (\"Bob\", 4000, \"Finance\"),\n",
    "         (\"Charlie\", 5000, \"IT\")]\n",
    "\n",
    "# Sample data for DataFrame 2\n",
    "data2 = [(\"Alice\", \"New York\"),\n",
    "         (\"Bob\", \"Los Angeles\"),\n",
    "         (\"David\", \"Chicago\")]\n",
    "\n",
    "# Creating DataFrames\n",
    "df1 = spark.createDataFrame(data1, [\"name\", \"salary\", \"department\"])\n",
    "df2 = spark.createDataFrame(data2, [\"name\", \"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|   name|salary|department|\n",
      "+-------+------+----------+\n",
      "|  Alice|  3000|        HR|\n",
      "|    Bob|  4000|   Finance|\n",
      "|Charlie|  5000|        IT|\n",
      "+-------+------+----------+\n",
      "\n",
      "+-----+-----------+\n",
      "| name|       city|\n",
      "+-----+-----------+\n",
      "|Alice|   New York|\n",
      "|  Bob|Los Angeles|\n",
      "|David|    Chicago|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inner join different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+-----------+\n",
      "| name|salary|department|       city|\n",
      "+-----+------+----------+-----------+\n",
      "|Alice|  3000|        HR|   New York|\n",
      "|  Bob|  4000|   Finance|Los Angeles|\n",
      "+-----+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform inner join on the 'name' column\n",
    "df_inner = df1.join(df2, on=\"name\", how=\"inner\")\n",
    "df_inner.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+-----+-----------+\n",
      "| name|salary|department| name|       city|\n",
      "+-----+------+----------+-----+-----------+\n",
      "|Alice|  3000|        HR|Alice|   New York|\n",
      "|  Bob|  4000|   Finance|  Bob|Los Angeles|\n",
      "+-----+------+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform inner join on the 'name' column\n",
    "df_inner = df1.join(df2, df1.name==df2.name,'inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left join different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-----------+\n",
      "|   name|salary|department|       city|\n",
      "+-------+------+----------+-----------+\n",
      "|  Alice|  3000|        HR|   New York|\n",
      "|    Bob|  4000|   Finance|Los Angeles|\n",
      "|Charlie|  5000|        IT|       null|\n",
      "+-------+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform left outer join on the 'name' column\n",
    "df_left = df1.join(df2, on=\"name\", how=\"left\")\n",
    "\n",
    "df_left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+-----+-----------+\n",
      "|   name|salary|department| name|       city|\n",
      "+-------+------+----------+-----+-----------+\n",
      "|  Alice|  3000|        HR|Alice|   New York|\n",
      "|    Bob|  4000|   Finance|  Bob|Los Angeles|\n",
      "|Charlie|  5000|        IT| null|       null|\n",
      "+-------+------+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform left outer join on the 'name' column\n",
    "df_left = df1.join(df2,df1.name==df2.name,'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full and right joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+-----+-----------+\n",
      "| name|salary|department| name|       city|\n",
      "+-----+------+----------+-----+-----------+\n",
      "|Alice|  3000|        HR|Alice|   New York|\n",
      "|  Bob|  4000|   Finance|  Bob|Los Angeles|\n",
      "| null|  null|      null|David|    Chicago|\n",
      "+-----+------+----------+-----+-----------+\n",
      "\n",
      "+-------+------+----------+-----+-----------+\n",
      "|   name|salary|department| name|       city|\n",
      "+-------+------+----------+-----+-----------+\n",
      "|  Alice|  3000|        HR|Alice|   New York|\n",
      "|    Bob|  4000|   Finance|  Bob|Los Angeles|\n",
      "|Charlie|  5000|        IT| null|       null|\n",
      "|   null|  null|      null|David|    Chicago|\n",
      "+-------+------+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_right = df1.join(df2, df1.name==df2.name,'right')\n",
    "df_right.show()\n",
    "\n",
    "df_full = df1.join(df2,df1.name==df2.name,'full')\n",
    "df_full.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "| name|salary|department|\n",
      "+-----+------+----------+\n",
      "|Alice|  3000|        HR|\n",
      "|  Bob|  4000|   Finance|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_semi = df1.join(df2, df1.name==df2.name,'left_semi')\n",
    "df_left_semi.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|   name|salary|department|\n",
      "+-------+------+----------+\n",
      "|Charlie|  5000|        IT|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_anti = df1.join(df2, df1.name==df2.name,'left_anti')\n",
    "df_left_anti.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|  name1|  name2|department|\n",
      "+-------+-------+----------+\n",
      "|    Bob|    Bob|   Finance|\n",
      "|  Alice|  Alice|        HR|\n",
      "|Charlie|Charlie|        IT|\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_self_join = df1.alias(\"df1\").join(\n",
    "    df1.alias(\"df2\"),\n",
    "    col('df1.department') == col('df2.department'),\n",
    "    'inner'\n",
    ").select(\n",
    "    col(\"df1.name\").alias(\"name1\"),\n",
    "    col(\"df2.name\").alias(\"name2\"),\n",
    "    col(\"df1.department\")\n",
    ")\n",
    "\n",
    "df_self_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|department|\n",
      "+----+----------+\n",
      "|John|        HR|\n",
      "|Jane|   Finance|\n",
      "| Joe|        IT|\n",
      "|Jill|        HR|\n",
      "+----+----------+\n",
      "\n",
      "+----------+--------------------+\n",
      "|department|     department_name|\n",
      "+----------+--------------------+\n",
      "|        HR|     Human Resources|\n",
      "|   Finance|Financial Department|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "\n",
    "# Sample large DataFrame\n",
    "data_large = [(\"John\", \"HR\"), (\"Jane\", \"Finance\"), (\"Joe\", \"IT\"), (\"Jill\", \"HR\")]\n",
    "columns_large = [\"name\", \"department\"]\n",
    "df_large = spark.createDataFrame(data_large, columns_large)\n",
    "\n",
    "# Sample small DataFrame\n",
    "data_small = [(\"HR\", \"Human Resources\"), (\"Finance\", \"Financial Department\")]\n",
    "columns_small = [\"department\", \"department_name\"]\n",
    "df_small = spark.createDataFrame(data_small, columns_small)\n",
    "\n",
    "df_large.show()\n",
    "\n",
    "df_small.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------------------+\n",
      "|name|department|department|     department_name|\n",
      "+----+----------+----------+--------------------+\n",
      "|John|        HR|        HR|     Human Resources|\n",
      "|Jane|   Finance|   Finance|Financial Department|\n",
      "|Jill|        HR|        HR|     Human Resources|\n",
      "+----+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast,col\n",
    "\n",
    "df_broadcasted = df_large.join(\n",
    "    broadcast(df_small),\n",
    "    df_large[\"department\"] == df_small[\"department\"],\n",
    "    \"inner\"\n",
    ")\n",
    "# Show the results\n",
    "df_broadcasted.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_project_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
