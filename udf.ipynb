{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"UDFExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+-----+\n",
      "| Name|Age|Department|salary|bonus|\n",
      "+-----+---+----------+------+-----+\n",
      "| John| 28|     Sales| 20000|   23|\n",
      "| Jane| 33| Marketing|  3000|   24|\n",
      "| Jake| 29|   Finance|  4000|   25|\n",
      "|Julie| 35|        HR|  5000|   26|\n",
      "+-----+---+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 28, \"Sales\",20000,23),\n",
    "        (\"Jane\", 33, \"Marketing\",3000,24),\n",
    "        (\"Jake\", 29, \"Finance\",4000,25),\n",
    "        (\"Julie\", 35, \"HR\",5000,26)]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Department\",\"salary\",\"bonus\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+-----+------+\n",
      "| Name|Age|Department|salary|bonus|totpay|\n",
      "+-----+---+----------+------+-----+------+\n",
      "| John| 28|     Sales| 20000|   23| 20023|\n",
      "| Jane| 33| Marketing|  3000|   24|  3024|\n",
      "| Jake| 29|   Finance|  4000|   25|  4025|\n",
      "|Julie| 35|        HR|  5000|   26|  5026|\n",
      "+-----+---+----------+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def total_pay(s,b):\n",
    "    return s+b\n",
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "total_payment = udf(lambda s,b:total_pay(s,b),IntegerType()) #registering udf\n",
    "\n",
    "df.withColumn('totpay',total_payment(df.salary,df.bonus)).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+-----+-----+\n",
      "| Name|Age|Department|salary|bonus| tpay|\n",
      "+-----+---+----------+------+-----+-----+\n",
      "| John| 28|     Sales| 20000|   23|20023|\n",
      "| Jane| 33| Marketing|  3000|   24| 3024|\n",
      "| Jake| 29|   Finance|  4000|   25| 4025|\n",
      "|Julie| 35|        HR|  5000|   26| 5026|\n",
      "+-----+---+----------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(returnType=IntegerType()) #registering udf\n",
    "\n",
    "def total_pay(s,b):\n",
    "    return s+b\n",
    "\n",
    "df.select('*',total_pay(df.salary,df.bonus).alias('tpay')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"emps\")\n",
    "\n",
    "\n",
    "spark.udf.register(name='tpay',f='total_pay',returnType=IntegerType())\n",
    "\n",
    "%sql\n",
    "select id,name,tpay(salary,bonus) from emps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complex data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  numbers|\n",
      "+---------+\n",
      "|[1, 2, 3]|\n",
      "|[4, 5, 6]|\n",
      "+---------+\n",
      "\n",
      "+---------+---------------+\n",
      "|  numbers|doubled_numbers|\n",
      "+---------+---------------+\n",
      "|[1, 2, 3]|      [2, 4, 6]|\n",
      "|[4, 5, 6]|    [8, 10, 12]|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType,IntegerType\n",
    "\n",
    "# Define a Python function that operates on arrays\n",
    "def double_elements(arr):\n",
    "    return [x * 2 for x in arr]\n",
    "\n",
    "# Register the UDF\n",
    "double_elements_udf = udf(double_elements, ArrayType(IntegerType()))\n",
    "\n",
    "# Create a DataFrame with an array column\n",
    "data = [([1, 2, 3],), ([4, 5, 6],)]\n",
    "columns = [\"numbers\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Apply the UDF\n",
    "df_transformed = df.withColumn(\"doubled_numbers\", double_elements_udf(col(\"numbers\")))\n",
    "\n",
    "# Show the results\n",
    "df_transformed.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_project_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
